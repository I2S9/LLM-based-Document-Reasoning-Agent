{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Notebook\n",
        "\n",
        "This notebook is for evaluating and benchmarking the document reasoning agent.\n",
        "\n",
        "## Features:\n",
        "- Metric calculation\n",
        "- Benchmark execution\n",
        "- Model comparison\n",
        "- Results visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path().absolute().parent))\n",
        "\n",
        "from src.evaluation.metrics import simple_similarity, chunk_relevance, measure_latency\n",
        "from src.evaluation.benchmark import Benchmark\n",
        "from src.agent.agent import Agent\n",
        "from src.agent.planner import plan\n",
        "from src.agent.worker import Worker\n",
        "from src.retrieval.retriever import Retriever\n",
        "from src.retrieval.chunker import chunk_text\n",
        "from src.llm.local_model_client import LocalModelClient\n",
        "\n",
        "# Configure matplotlib for inline display\n",
        "%matplotlib inline\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Metric Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test similarity metric\n",
        "text1 = \"Machine learning is a subset of artificial intelligence\"\n",
        "text2 = \"Machine learning uses algorithms to learn from data\"\n",
        "similarity = simple_similarity(text1, text2)\n",
        "print(f\"Text 1: '{text1}'\")\n",
        "print(f\"Text 2: '{text2}'\")\n",
        "print(f\"Similarity: {similarity:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize similarity comparison\n",
        "texts = [\n",
        "    (\"Machine learning is AI\", \"Machine learning uses algorithms\"),\n",
        "    (\"Deep learning uses neural networks\", \"Neural networks process data\"),\n",
        "    (\"NLP handles text\", \"Natural language processing analyzes text\")\n",
        "]\n",
        "\n",
        "similarities = [simple_similarity(t1, t2) for t1, t2 in texts]\n",
        "labels = [f\"Pair {i+1}\" for i in range(len(texts))]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(labels, similarities, color=['steelblue', 'coral', 'teal'], alpha=0.7)\n",
        "plt.ylabel('Similarity Score', fontsize=12)\n",
        "plt.title('Text Similarity Comparison', fontsize=14)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, sim in zip(bars, similarities):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f'{sim:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display as table\n",
        "df = pd.DataFrame({\n",
        "    'Text Pair': [f\"Pair {i+1}\" for i in range(len(texts))],\n",
        "    'Similarity': similarities\n",
        "})\n",
        "print(\"\\nSimilarity Results:\")\n",
        "print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test chunk relevance\n",
        "chunks = [\n",
        "    \"Machine learning is a subset of AI\",\n",
        "    \"Deep learning uses neural networks\",\n",
        "    \"Natural language processing handles text\"\n",
        "]\n",
        "query = \"machine learning artificial intelligence\"\n",
        "relevance = chunk_relevance(chunks, query)\n",
        "print(f\"Query: '{query}'\")\n",
        "print(f\"Chunks: {len(chunks)}\")\n",
        "print(f\"Average relevance: {relevance:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize chunk relevance\n",
        "chunks = [\n",
        "    \"Machine learning is a subset of AI\",\n",
        "    \"Deep learning uses neural networks\",\n",
        "    \"Natural language processing handles text\",\n",
        "    \"Computer vision processes images\"\n",
        "]\n",
        "queries = [\"machine learning\", \"neural networks\", \"text processing\"]\n",
        "\n",
        "relevance_data = []\n",
        "for query in queries:\n",
        "    relevance = chunk_relevance(chunks, query)\n",
        "    relevance_data.append(relevance)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(queries, relevance_data, color=['steelblue', 'coral', 'teal'], alpha=0.7)\n",
        "plt.ylabel('Average Relevance Score', fontsize=12)\n",
        "plt.xlabel('Query', fontsize=12)\n",
        "plt.title('Chunk Relevance by Query', fontsize=14)\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, rel in zip(bars, relevance_data):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f'{rel:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nRelevance Scores:\")\n",
        "for query, rel in zip(queries, relevance_data):\n",
        "    print(f\"  {query}: {rel:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Benchmark Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup for benchmark\n",
        "retriever = Retriever()\n",
        "test_text = \"\"\"\n",
        "Machine learning is a subset of artificial intelligence.\n",
        "Deep learning uses neural networks with multiple layers.\n",
        "Natural language processing handles text data.\n",
        "Supervised learning uses labeled data to train models.\n",
        "\"\"\"\n",
        "chunks = chunk_text(test_text, size=50)\n",
        "retriever.index_chunks(chunks)\n",
        "\n",
        "model = LocalModelClient()\n",
        "worker = Worker(retriever, model)\n",
        "agent = Agent(plan, worker, model)\n",
        "\n",
        "benchmark = Benchmark(agent, retriever)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"What is machine learning?\",\n",
        "        \"ground_truth\": \"Machine learning is a subset of artificial intelligence\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is deep learning?\",\n",
        "        \"ground_truth\": \"Deep learning uses neural networks with multiple layers\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is supervised learning?\",\n",
        "        \"ground_truth\": \"Supervised learning uses labeled data to train models\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Test cases: {len(test_cases)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and visualize benchmark results\n",
        "results_file = Path().absolute().parent / \"experiments\" / \"results.json\"\n",
        "if results_file.exists():\n",
        "    with open(results_file, 'r') as f:\n",
        "        results_data = json.load(f)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df_results = pd.DataFrame(results_data['results'])\n",
        "    \n",
        "    # Visualize metrics\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Latency by query\n",
        "    axes[0, 0].bar(range(len(df_results)), df_results['latency'], color='steelblue', alpha=0.7)\n",
        "    axes[0, 0].set_xlabel('Query Index')\n",
        "    axes[0, 0].set_ylabel('Latency (seconds)')\n",
        "    axes[0, 0].set_title('Latency by Query')\n",
        "    axes[0, 0].set_xticks(range(len(df_results)))\n",
        "    axes[0, 0].set_xticklabels([f\"Q{i+1}\" for i in range(len(df_results))])\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Similarity scores\n",
        "    axes[0, 1].bar(range(len(df_results)), df_results['score'], color='coral', alpha=0.7)\n",
        "    axes[0, 1].set_xlabel('Query Index')\n",
        "    axes[0, 1].set_ylabel('Similarity Score')\n",
        "    axes[0, 1].set_title('Similarity Scores by Query')\n",
        "    axes[0, 1].set_xticks(range(len(df_results)))\n",
        "    axes[0, 1].set_xticklabels([f\"Q{i+1}\" for i in range(len(df_results))])\n",
        "    axes[0, 1].set_ylim(0, 1)\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Chunk relevance\n",
        "    axes[1, 0].bar(range(len(df_results)), df_results['chunk_relevance'], color='teal', alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Query Index')\n",
        "    axes[1, 0].set_ylabel('Chunk Relevance')\n",
        "    axes[1, 0].set_title('Chunk Relevance by Query')\n",
        "    axes[1, 0].set_xticks(range(len(df_results)))\n",
        "    axes[1, 0].set_xticklabels([f\"Q{i+1}\" for i in range(len(df_results))])\n",
        "    axes[1, 0].set_ylim(0, 1)\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Number of chunks\n",
        "    axes[1, 1].bar(range(len(df_results)), df_results['num_chunks'], color='purple', alpha=0.7)\n",
        "    axes[1, 1].set_xlabel('Query Index')\n",
        "    axes[1, 1].set_ylabel('Number of Chunks')\n",
        "    axes[1, 1].set_title('Number of Retrieved Chunks')\n",
        "    axes[1, 1].set_xticks(range(len(df_results)))\n",
        "    axes[1, 1].set_xticklabels([f\"Q{i+1}\" for i in range(len(df_results))])\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Summary statistics table\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 60)\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Metric': ['Latency (s)', 'Similarity', 'Chunk Relevance', 'Num Chunks'],\n",
        "        'Mean': [\n",
        "            df_results['latency'].mean(),\n",
        "            df_results['score'].mean(),\n",
        "            df_results['chunk_relevance'].mean(),\n",
        "            df_results['num_chunks'].mean()\n",
        "        ],\n",
        "        'Std': [\n",
        "            df_results['latency'].std(),\n",
        "            df_results['score'].std(),\n",
        "            df_results['chunk_relevance'].std(),\n",
        "            df_results['num_chunks'].std()\n",
        "        ],\n",
        "        'Min': [\n",
        "            df_results['latency'].min(),\n",
        "            df_results['score'].min(),\n",
        "            df_results['chunk_relevance'].min(),\n",
        "            df_results['num_chunks'].min()\n",
        "        ],\n",
        "        'Max': [\n",
        "            df_results['latency'].max(),\n",
        "            df_results['score'].max(),\n",
        "            df_results['chunk_relevance'].max(),\n",
        "            df_results['num_chunks'].max()\n",
        "        ]\n",
        "    })\n",
        "    print(summary_df.to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"Results file not found. Run experiments/run_benchmark.py first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis between metrics\n",
        "if results_file.exists() and len(df_results) > 1:\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    \n",
        "    # Create correlation matrix\n",
        "    metrics = df_results[['latency', 'score', 'chunk_relevance', 'num_chunks']]\n",
        "    corr_matrix = metrics.corr()\n",
        "    \n",
        "    im = ax.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
        "    ax.set_xticks(range(len(corr_matrix.columns)))\n",
        "    ax.set_yticks(range(len(corr_matrix.columns)))\n",
        "    ax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
        "    ax.set_yticklabels(corr_matrix.columns)\n",
        "    ax.set_title('Metric Correlation Matrix', fontsize=14, pad=20)\n",
        "    \n",
        "    # Add text annotations\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(len(corr_matrix.columns)):\n",
        "            text = ax.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
        "                          ha=\"center\", va=\"center\", color=\"white\" if abs(corr_matrix.iloc[i, j]) > 0.5 else \"black\",\n",
        "                          fontsize=11, fontweight='bold')\n",
        "    \n",
        "    plt.colorbar(im, label='Correlation Coefficient')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nCorrelation Matrix:\")\n",
        "    print(corr_matrix.round(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run benchmark\n",
        "try:\n",
        "    summary = benchmark.run_benchmark(test_cases, k=3)\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"BENCHMARK RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Number of queries: {summary['num_queries']}\")\n",
        "    print(f\"Average latency: {summary['avg_latency']:.4f} seconds\")\n",
        "    print(f\"Average similarity: {summary['avg_similarity']:.3f}\")\n",
        "    print(f\"Average chunk relevance: {summary['avg_chunk_relevance']:.3f}\")\n",
        "    \n",
        "    print(\"\\nDetailed Results:\")\n",
        "    for i, result in enumerate(summary['results'], 1):\n",
        "        print(f\"\\nQuery {i}: '{result['query']}'\")\n",
        "        print(f\"  Latency: {result['latency']:.4f}s\")\n",
        "        print(f\"  Similarity: {result['similarity']:.3f}\")\n",
        "        print(f\"  Chunk relevance: {result['chunk_relevance']:.3f}\")\n",
        "        \n",
        "except NotImplementedError:\n",
        "    print(\"(Note: Local model not loaded - benchmark structure is correct)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
